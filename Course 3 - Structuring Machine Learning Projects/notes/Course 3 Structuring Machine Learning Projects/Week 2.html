<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Week 2</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

</style></head><body><article id="7a9abb6e-151f-4bb9-9909-3ee3f4b6d313" class="page sans"><header><h1 class="page-title">Week 2</h1><table class="properties"><tbody><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Class</th><td><span class="selected-value select-value-color-green">C3W2</span></td></tr><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCreatedAt"><path d="M6.98643729,14.0000972 C5.19579566,14.0000972 3.40419152,13.3106896 2.04245843,11.9323606 C-0.681017475,9.21200555 -0.680780251,4.76029539 2.04293482,2.04012507 C4.76664406,-0.68004331 9.22427509,-0.68004331 11.9480135,2.04013479 C13.272481,3.36277455 14,5.1330091 14,6.99552762 C14,8.87640182 13.2721894,10.6285043 11.9480135,11.9509302 C10.5679344,13.3105924 8.77756503,14.0000972 6.98643729,14.0000972 Z M10.2705296,7.00913883 L10.2705296,8.46099754 L10.2705296,8.65543362 L10.076181,8.65543362 L8.6543739,8.65543362 L5.72059514,8.65543362 L5.52619796,8.65543362 L5.52619796,8.46099754 L5.52619796,5.52541044 L5.52619796,3.37946773 L5.52619796,3.18502193 L5.72059514,3.18502193 L7.17253164,3.18502193 L7.36692883,3.18502193 L7.36692883,3.37946773 L7.36692883,6.81467358 L10.076181,6.81467358 L10.2705296,6.81467358 L10.2705296,7.00913883 Z M12.1601539,6.99552762 C12.1601539,5.61697497 11.6190112,4.32597154 10.6393933,3.34769528 C8.63253764,1.34336744 5.35197452,1.34061603 3.34153136,3.33944106 C3.33868273,3.34219247 3.33607716,3.34494388 3.33322852,3.34769528 C1.32397148,5.35459953 1.32372842,8.63641682 3.33322852,10.6433794 C5.34295224,12.6504489 8.62968901,12.6504489 10.6393933,10.6433794 C11.6190112,9.66506426 12.1601539,8.37408027 12.1601539,6.99552762 Z"></path></svg></span>Created</th><td><time>@Oct 11, 2020 7:39 AM</time></td></tr><tr class="property-row property-row-file"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesFile"><path d="M5.94578,14 C4.62416,14 3.38248,13.4963 2.44892,12.585 C1.514641,11.6736 1,10.4639 1,9.17405 C1.00086108,7.88562 1.514641,6.67434 2.44892,5.76378 L7.45612,0.985988 C8.80142,-0.327216 11.1777,-0.332396 12.5354,0.992848 C13.9369,2.36163 13.9369,4.58722 12.5354,5.95418 L8.03046,10.2414 C7.16278,11.0877 5.73682,11.0894 4.86024,10.2345 C3.98394,9.37789 3.98394,7.98769 4.86024,7.1327 L6.60422,5.4317 L7.87576,6.67196 L6.13177,8.37297 C6.01668,8.48539 6.00003,8.61545 6.00003,8.68335 C6.00003,8.75083 6.01668,8.88103 6.13177,8.99429 C6.36197,9.21689 6.53749,9.21689 6.76768,8.99429 L11.2707,4.70622 C11.9645,4.03016 11.9645,2.91757 11.2638,2.23311 C10.5843,1.57007 9.40045,1.57007 8.72077,2.23311 L3.71342,7.0109 C3.12602,7.58406 2.79837,8.35435 2.79837,9.17405 C2.79837,9.99459 3.12602,10.7654 3.72045,11.3446 C4.90947,12.5062 6.98195,12.5062 8.17096,11.3446 L10.41911,9.15165 L11.6906,10.3919 L9.4425,12.585 C8.50808,13.4963 7.2664,14 5.94578,14 Z"></path></svg></span>Materials</th><td></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>Property</th><td></td></tr><tr class="property-row property-row-checkbox"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCheckbox"><path d="M0,3 C0,1.34314 1.34326,0 3,0 L11,0 C12.6567,0 14,1.34314 14,3 L14,11 C14,12.6569 12.6567,14 11,14 L3,14 C1.34326,14 0,12.6569 0,11 L0,3 Z M3,1.5 C2.17139,1.5 1.5,2.17157 1.5,3 L1.5,11 C1.5,11.8284 2.17139,12.5 3,12.5 L11,12.5 C11.8286,12.5 12.5,11.8284 12.5,11 L12.5,3 C12.5,2.17157 11.8286,1.5 11,1.5 L3,1.5 Z M2.83252,6.8161 L3.39893,6.27399 L3.57617,6.10425 L3.92334,5.77216 L4.26904,6.10559 L4.44531,6.27582 L5.58398,7.37402 L9.28271,3.81073 L9.45996,3.64008 L9.80664,3.3056 L10.1538,3.63989 L10.3311,3.81067 L10.8936,4.35303 L11.0708,4.52399 L11.4434,4.88379 L11.0708,5.24353 L10.8936,5.41437 L6.1084,10.0291 L5.93115,10.2 L5.58398,10.5344 L5.23682,10.2 L5.05957,10.0292 L2.83057,7.87946 L2.65283,7.70801 L2.27832,7.34674 L2.6543,6.98694 L2.83252,6.8161 Z"></path></svg></span>Reviewed</th><td><div class="checkbox checkbox-off"></div></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Type</th><td><span class="selected-value select-value-color-yellow">Section</span></td></tr></tbody></table></header><div class="page-body"><h1 id="38ce32b9-c0a3-43de-b3ae-9bc7cb7f35d7" class="">Error Analysis</h1><h2 id="e26ba3b3-cd65-4fb3-9d20-9c0f164c29dd" class="">Carrying out error analysis.</h2><figure id="bc63e9f6-9646-4fff-8600-f73c0cf5900e" class="image"><a href="Week%202/Untitled.png"><img style="width:810px" src="Week%202/Untitled.png"/></a></figure><p id="32223e68-c34f-4659-9c58-8ad8110e520b" class="">In order to carry out error analysis, you should find a set of mislabeled examples either in your dev set or training set. Look at the mislabeled examples for false positives and false negatives. Count the number of errors that fall into various categories and during this process, you might be inspired to generate new categories of errors. By cointing up the fraction of examples that are mislabeled in dofferent way often this will help you priorotise or give you inspiration to pursue a new direction. This quick error counting procedure can help you make better priorotization decisions and understand how promising different approaches are to work on.</p><h2 id="cd90cb51-2d38-4cdd-af05-2ac6a1d37e71" class="">Cleaning up incorrectly labeled data.</h2><p id="4fd7e865-618d-4055-9ee3-98940831ffcc" class="">Data for a supervised learning problem comprises of input X and output labels Y. What if when going through the data you find that some of the output labels Y are incorrect meaning you have data which is incorrectly labeled, is it worth your while to gix up these labels?</p><ul id="297442f4-ae32-44d1-8bcf-f0ffcd056268" class="bulleted-list"><li>Consider the training set: DL algorithms are quite robust to <strong>random errors</strong> in the training set.</li></ul><ul id="6d2de7a9-ee86-41fc-98d1-bd71dcfd23fc" class="bulleted-list"><li>However, they are less robust to <strong>systematic error</strong>: If your classifier incorrectly labels white dogs as cats, then the problem is because your classifier will learn to classify all white colored dogs as cats and this is one of the current issue with biased DL trained models.</li></ul><p id="949b41fe-42fe-4575-ac5a-e888be9a538e" class="">
</p><p id="9be2f845-56d4-4bf0-bbe0-410bea3c0f16" class="">Recommendation: Create an error analysis table and add &quot;incorrectly labeled&quot; column.</p><figure id="4af7f479-2267-4b7d-87ee-eda7854ef909" class="image"><a href="Week%202/Untitled%201.png"><img style="width:801px" src="Week%202/Untitled%201.png"/></a></figure><p id="bc7f3972-673b-439b-bd31-6137caa99319" class="">Now looking at the image above, is it worthwhile spending more time trying to fix up the 6% of incorrectly labeled examples. Well, if it makes a significant difference to your ability to evaluate algorithms on your dev set, then consider spending more time to fix the incorect labels. One could start looking at:</p><ul id="080ef40f-ccbd-4f8d-ab6e-6c1959439452" class="bulleted-list"><li>Overral dev set error</li></ul><ul id="ca250c77-defb-4ad4-9fdc-791558679edd" class="bulleted-list"><li>Percentage of errors due to incorrect labels</li></ul><ul id="82bdf2ed-7efa-4bc6-8cfe-264c93ff6256" class="bulleted-list"><li>Errors due to other causes</li></ul><p id="4faf43be-26cf-4da3-b0de-e710d03c621d" class="">Note: The goal of the dev set is to help you select between two classifiers A &amp; B.</p><p id="4b7cf2f2-dd64-496e-94a6-0ea9178ebc9c" class="">
</p><p id="7e791eaf-805d-4854-908f-4795f05fb243" class="">Guidelines for correcting incorrect dev/test set examples.</p><ul id="b061b137-8819-4c62-808d-97cf903d4506" class="bulleted-list"><li>Apply the same process to your dev and test sets to make sure they continue to come from the same distribution.</li></ul><ul id="114896ff-e07a-402d-ba60-7a69188a3f2d" class="bulleted-list"><li>Consider examining examples your algorithm <strong>got right </strong>as well as ones it got <strong>wrong</strong>.</li></ul><ul id="062ddbd3-1088-409a-ab0f-53cd9d0f17eb" class="bulleted-list"><li>Train and dev/test data may now come from slightly different distributions. (It is super important for your dev/test set to come from the same distribution.)</li></ul><h2 id="fe468db2-b8db-4237-9b3f-42458c2c36a0" class="">Build your first system quickly, then iterate.</h2><p id="ff284cf7-3703-4d97-9889-eeb825fff0ae" class="">When working on a brand new machine learning application, one of the piece of advice often given is that, you should build your first system quickly and then iterate.</p><p id="9970fc8f-8563-4a95-ad19-5b372b1e4fde" class="">Key takeaways:</p><ul id="f4ecf8e4-42cd-4470-b66c-d26eef2dff13" class="bulleted-list"><li>Setup dev/test set and metric</li></ul><ul id="4e8d5fd2-4938-4e6c-958c-5c783f0bdab9" class="bulleted-list"><li>Build initial system quickly (Do not overthink or overcomplicate it)</li></ul><ul id="2fcbbe7d-522b-4ab7-b7d3-ffdde397b8b8" class="bulleted-list"><li>Use Bias/Variance analysis and Error analysis to priotitize next steps</li></ul><h1 id="ed721740-98c1-4acf-abcc-d94a3ad34086" class="">Mismatched training and dev/test set</h1><h2 id="dd385807-9209-41f8-8a5b-554726d8c13f" class="">Training and testing on different distributions</h2><p id="f7431b3e-d712-4ff3-97d6-3244fdb83c93" class="">Suppose you are building a cat classifier mobile app,where users will upload amatuer images from the mobile app. In order to train your model you would need to source images both from the uploaded app and some from the internet (as you need more training data) which could be high resolution and proffesional.</p><p id="ed69c14a-5b1c-4d55-ab81-9a486ae23a25" class="">Suppose you downloaded appx 200000 images from the internet and your users have only uploaded 10000 images(blurry and non prof) which your mobile app really cares about.</p><p id="fe0acc46-6461-4dcd-8458-56d2b3223042" class="">Now theres a dillema, as to you need your model to be trained on the same distribution of data but now you have data mismatch. Few guidelines with dealing with this issue:</p><ul id="e6371e41-1538-49f2-ae49-0bc8d586d0c8" class="bulleted-list"><li>Add and train a subset of uploaded images together with the images from the internet and use the randomly shuffled uploaded images for dev/test.</li></ul><ul id="f1f1459f-018e-483c-9d20-69f8c323e711" class="bulleted-list"><li>The advantage of splitting up your data into train, dev and test is that you&#x27;re now aiming the target where you want it to be and the dev set contains data uloaded from the mobile app which is the distribution that you really care about. So in essense your mobile app should do well with minor error margings.</li></ul><ul id="bb2f45e3-0dbb-46be-a8dc-b619e6966478" class="bulleted-list"><li>The disadvantage is that your training distribution is different from your dev and test set distribution. </li></ul><p id="241e7ae5-35dc-4fd3-9a4d-4cedf2e6eb13" class="">Key take away, is that It turns out that the above splitting will give better perfomance.</p><figure id="3b65a8b2-270d-4ad4-a840-3f26721ebb80" class="image"><a href="Week%202/Untitled%202.png"><img style="width:795px" src="Week%202/Untitled%202.png"/></a></figure><h2 id="6b74b066-2338-4a2c-85b1-dde40e7461a3" class="">Bias and Variance with mismatched data distributions</h2><p id="8b7b7456-4f11-4486-9d9d-f7410ed08ae3" class="">Estimating the bias and varience of your learning algorithm really helps you prioritize what you work on next but the way you analyse bias and varience changes when your training set comes from a different distribution than your dev and test sets.</p><figure id="08fe3bed-d034-4265-b12e-0acacbf91907" class="image"><a href="Week%202/Untitled%203.png"><img style="width:795px" src="Week%202/Untitled%203.png"/></a></figure><p id="6ffb8e31-3f53-4cde-8223-6aaee6f6d5fa" class="">Suppose we have our cat classifier and we get near perfect human error perfomance (All human&#x27;s correctly identified the cats from the data), thus Bayes Optimal Error is nearly 0%. </p><p id="03c1ccad-5941-4c58-ad70-2b54f9e99de8" class="">In order to do error analysis one would usually look at the training error as well as the dev set error. In the image above these errors are set to:</p><ul id="f281b067-3f9a-4572-857e-1b520c6c4cb2" class="bulleted-list"><li>Training error: 1%</li></ul><ul id="e71a4c7a-983b-432b-a8fc-9e0c5af06bba" class="bulleted-list"><li>Dev error: 10%</li></ul><p id="c0b0f311-a2c0-498d-a08d-9a741e7e2cb5" class="">if both training and dev set data came from the same distribution we would say that we have a <strong>large varience problem</strong> (The algorithm is not generalizing well from training set to dev set - perhaps it was trained on high res images and dev contained low and blurry images).</p><p id="df0f67ef-9174-4271-bbc4-d0a284f63053" class="">
</p><p id="80240fca-b0cd-4059-b1d9-7b149d3b9ff0" class="">The problem with the analysis above is with the algorithm is that:</p><ul id="e235ac56-3090-4ca8-8bc2-5b4ed5997da0" class="bulleted-list"><li>Data in the dev set and training set come from different distributions</li></ul><ul id="25fe30e7-6bfa-4f58-8fea-390ea8a499a9" class="bulleted-list"><li>The algorithm saw data in the training set but not in the dev set.</li></ul><p id="ae02da69-b83e-4b80-91ee-2ee3c9213d56" class="">
</p><p id="f7a793a0-f75c-4b51-9d22-27b034f39093" class="">In order to clear the above effects above, we would need to create a new randomy selected set: Training-dev set, which is from the same distrubution as the training set but not used for training.</p><p id="23faaaf5-8318-48a5-a7db-e729f3eb4f5a" class="">
</p><p id="49f01d72-0c5e-42bd-8281-72f99d196dc5" class="">Then only train the algo on the training set and test on both training-dev set and dev set, thus minimizing the error rate.</p><p id="92131db1-156d-4ff8-b3bf-4a9a1960f861" class="">
</p><p id="ac4612d8-8d9b-443f-a46f-6d2c2d59285b" class="">Data mismatch occurs when your algorithm (as shown in the image above) error analysis between the training-dev set and dev set is high, due to the learning algorithm being trained only on the training set and it learned to do well on a different distribution than the distribution we really care about.</p><p id="02a9ab9c-ce86-4a9c-aedf-e55e52f29a5e" class="">
</p><figure id="eebf3a1c-961d-49b7-a6de-837471166a9c" class="image"><a href="Week%202/Untitled%204.png"><img style="width:803px" src="Week%202/Untitled%204.png"/></a></figure><p id="b20d79c5-6098-4a6c-93f5-e6f5e9738716" class="">Examining the human level, training, training-dev and dev/test error can give you a better understanding as to how your algorithm is doing. And we have also seen that usin training data that comes from different distributuon as a dev and test set this could give tiy a lot more data and therefore help the perfomance of your learning algorithm. But instead of just having bias and varience potential problems you end up introducing data mismatch problem.</p><h2 id="5fda4ef0-5b6e-4f8a-a512-086c150c96c6" class="">Addressing data mismatch</h2><p id="fa556460-b5cc-4b52-a945-09ecd69c4df3" class="">If your training set comes from a different distribution than your dev and test set and if error analysis shows that you have data mismatch problem, Guideline of things to try:</p><ul id="d56f88b1-83f8-473f-b15f-444977bbab36" class="bulleted-list"><li>Carry out manual error analysis to try to understand the difference between training and dev/test sets.</li></ul><ul id="01a78d1d-fbe9-44ad-b7e2-63d6e599b33e" class="bulleted-list"><li>Make training data more similar or collect more data similar to dev/test set (Using techniques such as Artificial data synthesis).</li></ul><h1 id="af573b06-f5b8-4c17-aea9-915e248ad746" class="">Learning from multiple tasks</h1><h2 id="1ae0f432-b4e5-41d4-9bda-e7902291857a" class="">Transfer learning</h2><p id="ac452b8a-c1a0-4980-9386-aadfc12beb2e" class="">
</p><figure id="87e7149d-bcf8-42bd-8963-81053be726d1" class="image"><a href="Week%202/Untitled%205.png"><img style="width:804px" src="Week%202/Untitled%205.png"/></a></figure><p id="ae88e164-ea2f-479c-8300-f2e328e84c01" class="">According to <a href="https://developers.google.com/machine-learning/glossary#transfer-learning">https://developers.google.com/machine-learning/glossary#transfer-learning</a>, Transfer learning is the process of transferring information from one machine learning task to another.</p><p id="aa6668d7-1ea9-4e1d-a10e-806baa82b4dd" class="">
For example, in multi-task learning, a single model solves multiple tasks,
such as a <a href="https://developers.google.com/machine-learning/glossary#deep_model"><strong>deep model</strong></a> that has different output nodes for
different tasks. Transfer learning might involve transferring knowledge
from the solution of a simpler task to a more complex one, or involve
transferring knowledge from a task where there is more data to one where
there is less data.</p><p id="6fabb44d-6fb0-4b33-a394-901662600502" class="">Most machine learning systems solve a <em>single</em> task. Transfer learning is a
baby step towards artificial intelligence in which a single program can solve
<em>multiple</em> tasks.</p><p id="47418fa4-0b77-4210-8b5e-e6759ec9c98b" class="">
</p><p id="e996a477-176f-4dff-96e7-27c6aca75b65" class=""><strong>When transfer learning makes sense?</strong></p><ul id="d0fde055-53e0-4996-b7f1-15bef1fb0450" class="bulleted-list"><li>Task A and B have the same input X (images).</li></ul><ul id="140b0a05-09d0-47ea-b0d8-ed0f15f73ad7" class="bulleted-list"><li>You have a lot more data for Task A than Task B.</li></ul><ul id="7c2f0688-6480-42f4-abf6-98ecb84a667b" class="bulleted-list"><li>Low level features from A could be helpful for learning B.</li></ul><p id="15d0b99e-3535-460b-abd8-4f7fd780b665" class="">Transfer learning has been most useful if you&#x27;re trying to do well on some Task B, usually a problem where you have relatively little data. So for example, in radiology, you know it&#x27;s difficult to get that many x-ray scans to build a good radiology diagnosis system. So in that case, you might find a related but different task, such as image recognition, where you can get maybe a million images and learn a lot of load-over features from that, so that you can then try to do well on Task B on your radiology task despite not having that much data for it.</p><h2 id="920d1b8d-8638-4fa7-9f43-c1e551c7cd19" class="">Multi-task learning</h2><p id="12598bae-4e36-4aa7-8e4a-d4deb5782beb" class="">In multi-task learning, you start off simultaneously trying to have one neural network do several things at the same time and then each of these task helps all of the other tasks this improves better perfomance as compared to training individual nn in isolation.</p><figure id="ef8e75f5-a626-4d02-bfc2-6f56a5a2f0e4" class="image"><a href="Week%202/Untitled%206.png"><img style="width:799px" src="Week%202/Untitled%206.png"/></a></figure><p id="1e5ea5d0-b190-44af-8cd8-5f4bec27ef37" class="">An example would be a simplified autonomous driving computer vision algorithm that detects pedestrians, cars, stop signs and traffic lights. You could train a nn that would output matrix <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span></span><span>﻿</span></span> with dimensions <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(4, m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> instead of the default <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1, m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></p><figure id="0118891b-70d4-406d-911a-fd702f7b461a" class="image"><a href="Week%202/Untitled%207.png"><img style="width:793px" src="Week%202/Untitled%207.png"/></a></figure><p id="eb32b639-fd03-4e14-933e-795727c73ca6" class="">Unlike a softmax regression the nn architecture above can have multiple labels, so your nn architecture will have multiple probabilities i.e multiple classes/labels. </p><p id="7e0a0f9b-68ee-47b1-93cd-9577c494bfb0" class="">
</p><p id="4551a828-3f04-40ab-9886-032aeb5cdfaa" class=""><strong>When does multitask learning make sense?</strong></p><ul id="a10e880f-1091-4e15-92c7-9eefb1f36e74" class="bulleted-list"><li>Trainig on a set of tasks that could benefit from having shared lower-level features</li></ul><ul id="caa1c17f-496e-448d-93fe-876af397da04" class="bulleted-list"><li>Usually: Amount of data you have for each task is quite similar.</li></ul><ul id="e36f0f15-c5a0-4cbf-887f-b99502148206" class="bulleted-list"><li>Can train a big enough neural network to do well on all tasks. (Alternatively, training individual nn for each task can lead to perfomance degradation)</li></ul><h1 id="8161e77f-0a4c-406d-96fa-dd58b918ddb9" class="">End-to-end deep learning</h1><h2 id="ea0ab1d9-91fa-49f0-be82-d6695b114ce6" class="">What is end-to-end deep learning?</h2><p id="f11ff584-1850-4e64-92be-86624cd6dbe6" class="">What end-to-end deep learning does is ot can take multiple stages and replace them usually with just a single nn.</p><p id="1114e2b0-eb3a-4bb2-99c4-fc3eb0e3baca" class="">
</p><ol id="7c8a6855-c566-4723-9a7a-09e0e49b1972" class="numbered-list" start="1"><li>Multi-stage nn system<figure id="4be8c7f1-a357-495b-845a-5d880d618855" class="image"><a href="Week%202/Untitled%208.png"><img style="width:794px" src="Week%202/Untitled%208.png"/></a></figure><p id="9bbea26d-0b5f-4cf5-9ced-55bbab7e4b05" class="">Let&#x27;s take face recognition for example, breaking this application down into multistages increases better system perfomance were you detect a persons face then crop it and use it to match with faces in the database. This approach will not work efficiently on end-to-end deep learning unless there is large amount of data to compare to.</p></li></ol><ol id="e4d90f4d-bb97-4f01-a4bd-8b06ec50b7f0" class="numbered-list" start="2"><li>End-to-end deep learning<figure id="5d91d9c4-8d92-4589-b04a-51ea47f66475" class="image"><a href="Week%202/Untitled%209.png"><img style="width:804px" src="Week%202/Untitled%209.png"/></a></figure><p id="6eeee2b6-8d42-4743-a8d5-69d4c7e550ae" class="">In this example, machine translation suppose translatiing from English to French works well with end-to-end mapping as compared to the native way of multistaging were it would go through feature extraction(data to number values) and so on.</p><p id="b6acee4a-4572-491d-8a96-41e1f5fdf4fb" class="">The reason it works well, it&#x27;s due to the large data sets of X-Y pairs where the English words correspond to French.</p></li></ol><h2 id="c44f2aa6-a7ca-490c-9871-cc3a9a5db239" class="">Whether to use end-to-end deep learning</h2><p id="9c6a2a47-056d-434f-9983-f4ede1fb15e8" class=""><strong>Pros and cons of end-to-end deep learning</strong></p><p id="b66fee2a-26ba-4b99-8a0b-02ed8c2cb392" class="">Pros:</p><ul id="000b0bb4-38a5-4dd6-ae82-32896541c3e6" class="bulleted-list"><li>Let the data speak.</li></ul><ul id="101a5e4f-667f-4071-8103-d4e35c33e11f" class="bulleted-list"><li>Less hand-designing of components needed.</li></ul><p id="2d3b2a9f-5bf5-4811-8c4a-a85dbc20360a" class="">Cons:</p><ul id="620912a0-e851-4d63-83bf-089bd73b4383" class="bulleted-list"><li>May need a large amount of data.</li></ul><ul id="26d8a3cf-0d9d-4e7f-82c1-7d3b5c6d4c8c" class="bulleted-list"><li>Excludes potentially useful hand-designed components.</li></ul><p id="ec8fe567-76d3-49c9-9cae-16915cd1f073" class="">
</p><p id="46cb5317-ef89-4a6b-88e3-82d212394d65" class=""><strong>When do you apply end-to-end deep learning?</strong></p><p id="97648c4c-93bd-49c7-be1e-2af6021983bb" class="">Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y?</p><p id="735b6432-aac0-4140-975c-e6c86650b9a2" class="">
</p><hr id="06df1f94-7144-4c99-a5cc-8db32373e902"/><h1 id="d89f1d42-4a7f-425b-96d3-7e2cd7b428dc" class="">Q &amp; A</h1><h2 id="21a61b66-786c-4cab-b71b-8955943a4fdc" class="">Autonomous driving (case study)</h2><ol id="ab94e4aa-8783-4d98-8ac0-fe93f64cf3ff" class="numbered-list" start="1"><li>To help you practice strategies for machine learning, in this week we’ll present another scenario and ask how you would act. We think this “simulator” of working in a machine learning project will give a task of what leading a machine learning project could be like!<p id="9f25baf0-fec6-4e62-b511-3013991c235f" class="">You are employed by a startup building self-driving cars. You are in charge of detecting road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. As an example, the above image contains a pedestrian crossing sign and red traffic lights</p><p id="2326f1f1-8ab0-4055-ad90-335fff395ed8" class="">
</p><figure id="a3c9548d-c999-4fcb-b34d-dc8b985d2548" class="image"><a href="Week%202/Untitled%2010.png"><img style="width:1308px" src="Week%202/Untitled%2010.png"/></a></figure><p id="993bc7bd-6b9b-4432-a67f-6dcc339652ff" class="">Your 100,000 labeled images are taken using the front-facing camera of your car. This is also the distribution of data you care most about doing well on. You think you might be able to get a much larger dataset off the internet, that could be helpful for training even if the distribution of internet data is not the same.</p><p id="cbb62320-6a52-4eb9-864a-92247a849803" class="">You are just getting started on this project. What is the first thing you do? Assume each of the steps below would take about an equal amount of time (a few days).</p><ul id="810047dd-55e1-4f7a-837e-3406550b519a" class="bulleted-list"><li>Spend a few days training a basic model and see what mistakes it makes.<p id="71bb8674-d058-48bd-9d9e-b803f8c1b7b3" class=""><em>Justification:</em> As discussed in lecture, applied ML is a highly iterative process. If you train a basic model and carry out error analysis (see what mistakes it makes) it will help point you in more promising directions.</p></li></ul></li></ol><ol id="9edc9cac-fd2e-4973-af90-5d8d48b36381" class="numbered-list" start="2"><li>Your goal is to detect road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. You plan to use a deep neural network with ReLU units in the hidden layers.<p id="061d9cd4-91b4-4913-80f9-06814d551f4e" class="">For the output layer, a softmax activation would be a good choice for the output layer because this is a multi-task learning problem. True/False?</p><ul id="56477fae-0679-4e6e-a6c2-b463f9c94c97" class="bulleted-list"><li>False<p id="57474e3a-ad37-4310-9952-aee3bcb13fe2" class=""><em>Justification: </em>Softmax would be a good choice if one and only one of the possibilities (stop sign, speed bump, pedestrian crossing, green light and red light) was present in each image.</p></li></ul></li></ol><ol id="23bf7a5d-4079-467d-9db3-992ff7c840bb" class="numbered-list" start="3"><li>You are carrying out error analysis and counting up what errors the algorithm makes. Which of these datasets do you think you should manually go through and carefully examine, one image at a time?<ul id="94093172-8f9a-448e-93b3-46feddd364ef" class="bulleted-list"><li>500 images on which the algorithm made a mistake<p id="5f5fd4c7-2f6d-407f-a99d-de495e7c604a" class="">Justification: Focus on images that the algorithm got wrong. Also, 500 is enough to give you a good initial sense of the error statistics. There’s probably no need to look at 10,000, which will take a long time.</p></li></ul></li></ol><ol id="fbf74613-a6cf-4b0d-a061-5d34d102c09b" class="numbered-list" start="4"><li>After working on the data for several weeks, your team ends up with the following data:<ul id="5e6e8eb7-b6af-417e-8058-cf893e59b69b" class="bulleted-list"><li>100,000 labeled images taken using the front-facing camera of your car.</li></ul><ul id="f51381da-d061-4a26-af10-92ea0249adf5" class="bulleted-list"><li>900,000 labeled images of roads downloaded from the internet.</li></ul><ul id="485ad882-ed87-498a-8a32-1c7c21c7301a" class="bulleted-list"><li>Each image’s labels precisely indicate the presence of any specific road signs and traffic signals or combinations of them. For example,<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mo stretchy="false">[</mo><mo stretchy="false">[</mo><mn>10010</mn><mo stretchy="false">]</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex"> y^{(i)} = [[10010]]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">[</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">]</span><span class="mclose">]</span></span></span></span></span><span>﻿</span></span> means the image contains a stop sign and a red traffic light.
Because this is a multi-task learning problem, you need to have all your <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">y^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>
vectors fully labeled. If one example is equal to<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">?</mo><mn>11</mn><mo stretchy="false">?</mo><mo stretchy="false">]</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[[0?11?]]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">[</span><span class="mord">0</span><span class="mclose">?</span><span class="mord">1</span><span class="mord">1</span><span class="mclose">?</span><span class="mclose">]</span><span class="mclose">]</span></span></span></span></span><span>﻿</span></span> then the learning algorithm will not be able to use that example. True/False?<ul id="8dc9fb5c-41ea-4ddf-884f-fea31f229ff1" class="bulleted-list"><li>False
<em>Justification:</em> As seen in the lecture on multi-task learning, you can compute the cost such that it is not influenced by the fact that some entries haven’t been labeled.</li></ul></li></ul><p id="292f369f-2c40-460b-86f1-4e42efecbe17" class="">
</p></li></ol><ol id="00f50eb6-28e6-4142-8dd9-01b27bb8d2c1" class="numbered-list" start="5"><li>The distribution of data you care about contains images from your car’s front-facing camera; which comes from a different distribution than the images you were able to find and download off the internet. How should you split the dataset into train/dev/test sets?<ul id="5f99981f-ffaf-4a3e-86ba-26951050de2c" class="bulleted-list"><li>Choose the training set to be the 900,000 images from the internet along with 80,000 images from your car’s front-facing camera. The 20,000 remaining images will be split equally in dev and test sets.<p id="490d3bad-f02c-4d29-9b25-0b37fdd757b8" class=""><em>Justification:</em> Yes. As seen in lecture, it is important that your dev and test set have the closest possible distribution to “real”-data. It is also important for the training set to contain enough “real”-data to avoid having a data-mismatch problem.</p><p id="bcf4e117-b776-434e-b412-dc6cce6b0eb8" class="">
</p></li></ul></li></ol><ol id="e29818a2-55b4-414c-8e68-d57ebe596f09" class="numbered-list" start="6"><li>Assume you’ve finally chosen the following split between of the data:<div id="0d572667-9b3d-47e9-a97d-de4995cd14de" class="collection-content"><h4 class="collection-title"></h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesTitle"><path d="M7.73943662,8.6971831 C7.77640845,8.7834507 7.81338028,8.8943662 7.81338028,9.00528169 C7.81338028,9.49823944 7.40669014,9.89260563 6.91373239,9.89260563 C6.53169014,9.89260563 6.19894366,9.64612676 6.08802817,9.30105634 L5.75528169,8.33978873 L2.05809859,8.33978873 L1.72535211,9.30105634 C1.61443662,9.64612676 1.2693662,9.89260563 0.887323944,9.89260563 C0.394366197,9.89260563 0,9.49823944 0,9.00528169 C0,8.8943662 0.0246478873,8.7834507 0.0616197183,8.6971831 L2.46478873,2.48591549 C2.68661972,1.90669014 3.24119718,1.5 3.90669014,1.5 C4.55985915,1.5 5.12676056,1.90669014 5.34859155,2.48591549 L7.73943662,8.6971831 Z M2.60035211,6.82394366 L5.21302817,6.82394366 L3.90669014,3.10211268 L2.60035211,6.82394366 Z M11.3996479,3.70598592 C12.7552817,3.70598592 14,4.24823944 14,5.96126761 L14,9.07922535 C14,9.52288732 13.6549296,9.89260563 13.2112676,9.89260563 C12.8169014,9.89260563 12.471831,9.59683099 12.4225352,9.19014085 C12.028169,9.6584507 11.3257042,9.95422535 10.5492958,9.95422535 C9.60035211,9.95422535 8.47887324,9.31338028 8.47887324,7.98239437 C8.47887324,6.58978873 9.60035211,6.08450704 10.5492958,6.08450704 C11.3380282,6.08450704 12.040493,6.33098592 12.4348592,6.81161972 L12.4348592,5.98591549 C12.4348592,5.38204225 11.9172535,4.98767606 11.1285211,4.98767606 C10.6602113,4.98767606 10.2411972,5.11091549 9.80985915,5.38204225 C9.72359155,5.43133803 9.61267606,5.46830986 9.50176056,5.46830986 C9.18133803,5.46830986 8.91021127,5.1971831 8.91021127,4.86443662 C8.91021127,4.64260563 9.0334507,4.44542254 9.19366197,4.34683099 C9.87147887,3.90316901 10.6232394,3.70598592 11.3996479,3.70598592 Z M11.1778169,8.8943662 C11.6830986,8.8943662 12.1760563,8.72183099 12.4348592,8.37676056 L12.4348592,7.63732394 C12.1760563,7.29225352 11.6830986,7.11971831 11.1778169,7.11971831 C10.5616197,7.11971831 10.056338,7.45246479 10.056338,8.0193662 C10.056338,8.57394366 10.5616197,8.8943662 11.1778169,8.8943662 Z M0.65625,11.125 L13.34375,11.125 C13.7061869,11.125 14,11.4188131 14,11.78125 C14,12.1436869 13.7061869,12.4375 13.34375,12.4375 L0.65625,12.4375 C0.293813133,12.4375 4.43857149e-17,12.1436869 0,11.78125 C-4.43857149e-17,11.4188131 0.293813133,11.125 0.65625,11.125 Z"></path></svg></span>Dataset</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Contains</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesNumber"><path d="M4.46191,0 C3.8667,0 3.38428,0.482422 3.38428,1.07751 L3.38428,3.38425 L1.07764,3.38425 C0.482422,3.38425 0,3.86667 0,4.46179 C0,5.05688 0.482422,5.53931 1.07764,5.53931 L3.38428,5.53931 L3.38428,8.46063 L1.07764,8.46063 C0.482422,8.46063 0,8.94308 0,9.53818 C0,10.1333 0.482422,10.6157 1.07764,10.6157 L3.38428,10.6157 L3.38428,12.9224 C3.38428,13.5175 3.8667,13.9999 4.46191,13.9999 C5.05664,13.9999 5.53906,13.5175 5.53906,12.9224 L5.53906,10.6157 L8.46045,10.6157 L8.46045,12.9224 C8.46045,13.5175 8.94287,13.9999 9.53809,13.9999 C10.1333,13.9999 10.6157,13.5175 10.6157,12.9224 L10.6157,10.6157 L12.9224,10.6157 C13.5176,10.6157 14,10.1333 14,9.53818 C14,8.94308 13.5176,8.46063 12.9224,8.46063 L10.6157,8.46063 L10.6157,5.53931 L12.9224,5.53931 C13.5176,5.53931 14,5.05688 14,4.46179 C14,3.86667 13.5176,3.38425 12.9224,3.38425 L10.6157,3.38425 L10.6157,1.07751 C10.6157,0.482422 10.1333,0 9.53809,0 C8.94287,0 8.46045,0.482422 8.46045,1.07751 L8.46045,3.38425 L5.53906,3.38425 L5.53906,1.07751 C5.53906,0.482422 5.05664,0 4.46191,0 Z M5.53906,8.46063 L5.53906,5.53931 L8.46045,5.53931 L8.46045,8.46063 L5.53906,8.46063 Z"></path></svg></span>Error of the algorithm</th></tr></thead><tbody><tr id="60c42048-7977-481f-9f21-17a7190f54fa"><td class="cell-title"><a href="Week%202/Untitled%20Database%200d5726679b3d47e9a97dde4995cd14de/Training%2060c420487977481f9f2117a7190f54fa.html">Training</a></td><td class="cell-J:xY">940000 images randomly picked from (900000 internet images + 60000 car’s front-facing camera images)</td><td class="cell-gXeJ">8.8</td></tr><tr id="e4746ef8-d6a7-4330-9009-ed892018d280"><td class="cell-title"><a href="Week%202/Untitled%20Database%200d5726679b3d47e9a97dde4995cd14de/Training-Dev%20e4746ef8d6a743309009ed892018d280.html">Training-Dev</a></td><td class="cell-J:xY">20,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)</td><td class="cell-gXeJ">9.1</td></tr><tr id="c713c7a2-a28b-481a-909d-3190bd60718c"><td class="cell-title"><a href="Week%202/Untitled%20Database%200d5726679b3d47e9a97dde4995cd14de/Dev%20c713c7a2a28b481a909d3190bd60718c.html">Dev</a></td><td class="cell-J:xY">20,000 images from your car’s front-facing camera </td><td class="cell-gXeJ">14.3</td></tr><tr id="7c2ca295-fcd8-4b4c-bbc6-538fcf5e106c"><td class="cell-title"><a href="Week%202/Untitled%20Database%200d5726679b3d47e9a97dde4995cd14de/Test%207c2ca295fcd84b4cbbc6538fcf5e106c.html">Test</a></td><td class="cell-J:xY">20,000 images from the car’s front-facing camera</td><td class="cell-gXeJ">14.8</td></tr></tbody></table></div><p id="8adbb9c3-975d-4563-af66-a27e66cca5fb" class="">You also know that human-level error on the road sign and traffic signals classification task is around 0.5%. Which of the following are True? (Check all that apply).</p><ul id="4924b18c-ada9-4d10-8e0a-e91ef6451445" class="bulleted-list"><li>You have a large data-mismatch problem because your model does a lot better on the training-dev set than on the dev set</li></ul><ul id="0f05b1a3-0f9f-4fa1-91de-144abb55fc89" class="bulleted-list"><li>You have a large avoidable-bias problem because your training error is quite a bit higher than the human-level error.</li></ul><p id="0899ed5f-a251-4311-aa4f-2028d492b4c4" class="">
</p></li></ol><ol id="4dcba227-43fb-4bcf-b002-90a0de127846" class="numbered-list" start="7"><li>Based on table from the previous question, a friend thinks that the training data distribution is much easier than the dev/test distribution. What do you think?<ul id="94e2e29c-9578-4b90-8040-b2b1f8720f86" class="bulleted-list"><li>There’s insufficient information to tell if your friend is right or wrong.<p id="9d4e567c-5a6b-49da-94e4-d59ad2b90891" class=""><em>Justification:</em> The algorithm does better on the distribution of data it trained on. But you don’t know if it’s because it trained on that no distribution or if it really is easier. To get a better sense, measure human-level error separately on both distributions.</p><p id="e5ce21ea-8f58-400a-9b7f-24713cd2e518" class="">
</p></li></ul></li></ol><ol id="c783fd1f-dfbf-44b7-87fb-27ca10147950" class="numbered-list" start="8"><li>You decide to focus on the dev set and check by hand what are the errors due to. Here is a table summarizing your discoveries:<div id="96de5624-94ab-44f9-b7a2-46ef38f7b02f" class="collection-content"><h4 class="collection-title"></h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesTitle"><path d="M7.73943662,8.6971831 C7.77640845,8.7834507 7.81338028,8.8943662 7.81338028,9.00528169 C7.81338028,9.49823944 7.40669014,9.89260563 6.91373239,9.89260563 C6.53169014,9.89260563 6.19894366,9.64612676 6.08802817,9.30105634 L5.75528169,8.33978873 L2.05809859,8.33978873 L1.72535211,9.30105634 C1.61443662,9.64612676 1.2693662,9.89260563 0.887323944,9.89260563 C0.394366197,9.89260563 0,9.49823944 0,9.00528169 C0,8.8943662 0.0246478873,8.7834507 0.0616197183,8.6971831 L2.46478873,2.48591549 C2.68661972,1.90669014 3.24119718,1.5 3.90669014,1.5 C4.55985915,1.5 5.12676056,1.90669014 5.34859155,2.48591549 L7.73943662,8.6971831 Z M2.60035211,6.82394366 L5.21302817,6.82394366 L3.90669014,3.10211268 L2.60035211,6.82394366 Z M11.3996479,3.70598592 C12.7552817,3.70598592 14,4.24823944 14,5.96126761 L14,9.07922535 C14,9.52288732 13.6549296,9.89260563 13.2112676,9.89260563 C12.8169014,9.89260563 12.471831,9.59683099 12.4225352,9.19014085 C12.028169,9.6584507 11.3257042,9.95422535 10.5492958,9.95422535 C9.60035211,9.95422535 8.47887324,9.31338028 8.47887324,7.98239437 C8.47887324,6.58978873 9.60035211,6.08450704 10.5492958,6.08450704 C11.3380282,6.08450704 12.040493,6.33098592 12.4348592,6.81161972 L12.4348592,5.98591549 C12.4348592,5.38204225 11.9172535,4.98767606 11.1285211,4.98767606 C10.6602113,4.98767606 10.2411972,5.11091549 9.80985915,5.38204225 C9.72359155,5.43133803 9.61267606,5.46830986 9.50176056,5.46830986 C9.18133803,5.46830986 8.91021127,5.1971831 8.91021127,4.86443662 C8.91021127,4.64260563 9.0334507,4.44542254 9.19366197,4.34683099 C9.87147887,3.90316901 10.6232394,3.70598592 11.3996479,3.70598592 Z M11.1778169,8.8943662 C11.6830986,8.8943662 12.1760563,8.72183099 12.4348592,8.37676056 L12.4348592,7.63732394 C12.1760563,7.29225352 11.6830986,7.11971831 11.1778169,7.11971831 C10.5616197,7.11971831 10.056338,7.45246479 10.056338,8.0193662 C10.056338,8.57394366 10.5616197,8.8943662 11.1778169,8.8943662 Z M0.65625,11.125 L13.34375,11.125 C13.7061869,11.125 14,11.4188131 14,11.78125 C14,12.1436869 13.7061869,12.4375 13.34375,12.4375 L0.65625,12.4375 C0.293813133,12.4375 4.43857149e-17,12.1436869 0,11.78125 C-4.43857149e-17,11.4188131 0.293813133,11.125 0.65625,11.125 Z"></path></svg></span>Error Analysis</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesNumber"><path d="M4.46191,0 C3.8667,0 3.38428,0.482422 3.38428,1.07751 L3.38428,3.38425 L1.07764,3.38425 C0.482422,3.38425 0,3.86667 0,4.46179 C0,5.05688 0.482422,5.53931 1.07764,5.53931 L3.38428,5.53931 L3.38428,8.46063 L1.07764,8.46063 C0.482422,8.46063 0,8.94308 0,9.53818 C0,10.1333 0.482422,10.6157 1.07764,10.6157 L3.38428,10.6157 L3.38428,12.9224 C3.38428,13.5175 3.8667,13.9999 4.46191,13.9999 C5.05664,13.9999 5.53906,13.5175 5.53906,12.9224 L5.53906,10.6157 L8.46045,10.6157 L8.46045,12.9224 C8.46045,13.5175 8.94287,13.9999 9.53809,13.9999 C10.1333,13.9999 10.6157,13.5175 10.6157,12.9224 L10.6157,10.6157 L12.9224,10.6157 C13.5176,10.6157 14,10.1333 14,9.53818 C14,8.94308 13.5176,8.46063 12.9224,8.46063 L10.6157,8.46063 L10.6157,5.53931 L12.9224,5.53931 C13.5176,5.53931 14,5.05688 14,4.46179 C14,3.86667 13.5176,3.38425 12.9224,3.38425 L10.6157,3.38425 L10.6157,1.07751 C10.6157,0.482422 10.1333,0 9.53809,0 C8.94287,0 8.46045,0.482422 8.46045,1.07751 L8.46045,3.38425 L5.53906,3.38425 L5.53906,1.07751 C5.53906,0.482422 5.05664,0 4.46191,0 Z M5.53906,8.46063 L5.53906,5.53931 L8.46045,5.53931 L8.46045,8.46063 L5.53906,8.46063 Z"></path></svg></span>Error Rate</th></tr></thead><tbody><tr id="1dcbdc1f-8d2f-4dbb-8fa9-f4c439ab5fd2"><td class="cell-title"><a href="Week%202/Untitled%20Database%2096de562494ab44f9b7a246ef38f7b02f/Overall%20dev%20set%20error%201dcbdc1f8d2f4dbb8fa9f4c439ab5fd2.html">Overall dev set error </a></td><td class="cell-iISh">15.3</td></tr><tr id="68c54174-3fd9-4fc2-a73e-05a6f9c14e8d"><td class="cell-title"><a href="Week%202/Untitled%20Database%2096de562494ab44f9b7a246ef38f7b02f/Errors%20due%20to%20incorrectly%20labeled%20data%2068c541743fd94fc2a73e05a6f9c14e8d.html">Errors due to incorrectly labeled data</a></td><td class="cell-iISh">4.1</td></tr><tr id="4342ec58-7102-44f3-9e0f-6c0926568284"><td class="cell-title"><a href="Week%202/Untitled%20Database%2096de562494ab44f9b7a246ef38f7b02f/Errors%20due%20to%20foggy%20pictures%204342ec58710244f39e0f6c0926568284.html">Errors due to foggy pictures</a></td><td class="cell-iISh">8</td></tr><tr id="db597294-b2f1-4ef9-9c19-bf116d01cee2"><td class="cell-title"><a href="Week%202/Untitled%20Database%2096de562494ab44f9b7a246ef38f7b02f/Errors%20due%20to%20rain%20drops%20stuck%20on%20your%20car%E2%80%99s%20front%20db597294b2f14ef99c19bf116d01cee2.html">Errors due to rain drops stuck on your car’s front-facing camera</a></td><td class="cell-iISh">2.2</td></tr><tr id="4c660963-d733-4935-adbc-192675482d83"><td class="cell-title"><a href="Week%202/Untitled%20Database%2096de562494ab44f9b7a246ef38f7b02f/Errors%20due%20to%20rain%20drops%20stuck%20on%20your%20car%E2%80%99s%20front%204c660963d7334935adbc192675482d83.html">Errors due to rain drops stuck on your car’s front-facing camera</a></td><td class="cell-iISh">1</td></tr></tbody></table></div><p id="f71d0c39-1af3-4291-9fc8-8842df94d80a" class="">
In this table, 4.1%, 8.0%, etc. are a fraction of the total dev set (not just examples your algorithm mislabeled). For example, about 8.0/15.3 = 52% of your errors are due to foggy pictures.</p><p id="79d6a9d8-7893-4bc0-8513-b2f14e1a0b46" class="">The results from this analysis implies that the team’s highest priority should be to bring more foggy pictures into the training set so as to address the 8.0% of errors in that category. True/False?</p><p id="e00357f9-cb50-4e54-8ac3-3994f880e05d" class="">Additional Note: there are subtle concepts to consider with this question, and you may find arguments for why some answers are also correct or incorrect. We recommend that you spend time reading the feedback for this quiz, to understand what issues that you will want to consider when you are building your own machine learning project.</p><ul id="b5ff4077-6595-4698-b66e-6b1e3a6d2faa" class="bulleted-list"><li>False because it depends on how easy it is to add foggy data. If foggy data is very hard and costly to collect, it might not be worth the team’s effort.
<em>Justification: </em>correct: feedback: This is the correct answer. You should consider the tradeoff between the data accessibility and potential improvement of your model trained on this additional data.
</li></ul></li></ol><ol id="29691142-8fef-4a38-b8ba-7139975c8859" class="numbered-list" start="9"><li>You can buy a specially designed windshield wiper that help wipe off some of the raindrops on the front-facing camera. Based on the table from the previous question, which of the following statements do you agree with?<ul id="efb78b4a-fe5c-4a37-b530-ee8a6d853b3b" class="bulleted-list"><li>2.2% would be a reasonable estimate of the maximum amount this windshield wiper could improve performance.<p id="7baec2e8-5f54-4774-8d1d-29a16f06eb6b" class=""><em>Justification: </em>Yes. You will probably not improve performance by more than 2.2% by solving the raindrops problem. If your dataset was infinitely big, 2.2% would be a perfect estimate of the improvement you can achieve by purchasing a specially designed windshield wiper that removes the raindrops.</p></li></ul><p id="711cfd9f-51a5-4b1d-8c6f-c951c92e0152" class="">
</p></li></ol><ol id="57656311-f6b9-4a54-bca3-cb627bfac71d" class="numbered-list" start="10"><li>You decide to use data augmentation to address foggy images. You find 1,000 pictures of fog off the internet, and “add” them to clean images to synthesize foggy days, like this:<p id="9b6837c8-66ea-4c9e-b758-522185639224" class="">
</p><figure id="b231234c-8f73-4e9e-9547-33eb24301fe9" class="image"><a href="Week%202/Untitled%2011.png"><img style="width:1390px" src="Week%202/Untitled%2011.png"/></a></figure><p id="e133c0b5-63d6-47f5-ae37-2fcff8b9c6df" class="">Which of the following statements do you agree with?</p><ul id="2cb596ac-caea-49ea-bae0-e3360e34b10a" class="bulleted-list"><li>So long as the synthesized fog looks realistic to the human eye, you can be confident that the synthesized data is accurately capturing the distribution of real foggy images (or a subset of it), since human vision is very accurate for the problem you’re solving.<p id="7d2c9791-bb42-458f-b4b6-0fcc9894b4c8" class=""><em>Justification: </em>Yes. If the synthesized images look realistic, then the model will just see them as if you had added useful data to identify road signs and traffic signals in a foggy weather. I will very likely help.</p></li></ul><p id="47b6e522-8817-4d68-8a8d-4a70c571c6b4" class="">
</p></li></ol><ol id="d9f1a99d-9e34-4498-bf95-27cb8f6fabd3" class="numbered-list" start="11"><li>After working further on the problem, you’ve decided to correct the incorrectly labeled data on the dev set. Which of these statements do you agree with? (Check all that apply).<ul id="1d8742c9-d7f1-4eea-bf81-99aca74a96b3" class="bulleted-list"><li>You should also correct the incorrectly labeled data in the test set, so that the dev and test sets continue to come from the same distribution<p id="7fcdedea-6789-4041-9fed-4e2daf90d048" class=""><em>Justification:</em> Yes because you want to make sure that your dev and test data come from the same distribution for your algorithm to make your team’s iterative development process is efficient.</p></li></ul><ul id="e2bfc393-7937-455b-9912-b09203a4adac" class="bulleted-list"><li>You do not necessarily need to fix the incorrectly labeled data in the training set, because it&#x27;s okay for the training set distribution to differ from the dev and test sets. Note that it is important that the dev set and test set have the same distribution.<p id="5658c2b1-a6b6-4ac2-91d2-6353c4926be9" class=""><em>Justification:</em> True, deep learning algorithms are quite robust to having slightly different train and dev distributions.</p><p id="9ffc73a8-7347-48fd-a99a-197778a9cd9e" class="">
</p></li></ul></li></ol><ol id="066fa075-089a-4102-9106-c843df8f19a9" class="numbered-list" start="12"><li>So far your algorithm only recognizes red and green traffic lights. One of your colleagues in the startup is starting to work on recognizing a yellow traffic light. (Some countries call it an orange light rather than a yellow light; we’ll use the US convention of calling it yellow.) Images containing yellow lights are quite rare, and she doesn’t have enough data to build a good model. She hopes you can help her out using transfer learning.<p id="cd72fe6c-0e38-4f9c-990d-11000a4296eb" class="">What do you tell your colleague?</p><ul id="6df448b0-1487-479e-b2bd-d60863033464" class="bulleted-list"><li>She should try using weights pre-trained on your dataset, and fine-tuning further with the yellow-light dataset.<p id="8370c02d-0a81-488a-a315-f673087cb656" class=""><em>Justification:</em> Yes. You have trained your model on a huge dataset, and she has a small dataset. Although your labels are different, the parameters of your model have been trained to recognize many characteristics of road and traffic images which will be useful for her problem. This is a perfect case for transfer learning, she can start with a model with the same architecture as yours, change what is after the last hidden layer and initialize it with your trained parameters.
</p></li></ul></li></ol><ol id="93c59746-cc93-4fc6-99f1-8abf7f43e1c3" class="numbered-list" start="13"><li>Another colleague wants to use microphones placed outside the car to better hear if there’re other vehicles around you. For example, if there is a police vehicle behind you, you would be able to hear their siren. However, they don’t have much to train this audio system. How can you help?<ul id="4a73e0d8-7e1d-4614-9fa2-ca71f9c1de8c" class="bulleted-list"><li>Neither transfer learning nor multi-task learning seems promising.<p id="f39cf0d1-4134-470f-8662-a6af3e55c6d1" class=""><em>Justification: </em>Yes. The problem he is trying to solve is quite different from yours. The different dataset structures make it probably impossible to use transfer learning or multi-task learning.</p></li></ul><p id="89884990-1944-411f-b281-36658fa85787" class="">
</p></li></ol><ol id="8dc0335e-5348-4034-a5e3-18a3a591dde4" class="numbered-list" start="14"><li>To recognize red and green lights, you have been using this approach:<p id="1c1f53e9-c6ba-4cfa-b144-0c93efb63682" class="">(A) Input an image (x) to a neural network and have it directly learn a mapping to make a prediction as to whether there’s a red light and/or green light (y).
A teammate proposes a different, two-step approach:</p><p id="7c85e511-46ee-4983-aa28-cff60ebaec92" class="">(B) In this two-step approach, you would first (i) detect the traffic light in the image (if any), then (ii) determine the color of the illuminated lamp in the traffic light.
Between these two, Approach B is more of an end-to-end approach because it has distinct steps for the input end and the output end. True/False?<div class="indented"><ul id="e214760e-d8b2-47fa-bff3-331c969195f8" class="bulleted-list"><li>False<p id="bb427570-3d26-43b9-baec-340b5e609ae1" class=""><em>Justification: </em>Yes. (A) is an end-to-end approach as it maps directly the input (x) to the output (y).</p></li></ul><p id="4a484120-504a-4836-be20-cd351fb53ec3" class="">
</p></div></p></li></ol><ol id="a72ddb8b-741e-4939-9e76-18d7cbd2e1e7" class="numbered-list" start="15"><li>Approach A (in the question above) tends to be more promising than approach B if you have a ________ (fill in the blank).<ul id="c416deb7-f771-47fd-863c-e2aab1d0da7f" class="bulleted-list"><li>Large training set<p id="e7d371d6-e01b-460a-850f-7ee05cf31e5d" class=""><em>Justification: </em>Yes. In many fields, it has been observed that end-to-end learning works better in practice, but requires a large amount of data.</p></li></ul></li></ol><p id="f76b4aad-b95a-45fd-a93f-2c28ee2b2601" class="">
</p></div></article></body></html>