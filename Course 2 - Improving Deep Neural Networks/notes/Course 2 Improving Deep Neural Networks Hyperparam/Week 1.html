<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Week 1</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

</style></head><body><article id="240ca72d-61fb-491b-b843-0f35d8ddfee8" class="page sans"><header><h1 class="page-title">Week 1</h1><table class="properties"><tbody><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Class</th><td><span class="selected-value select-value-color-red">C2W1</span></td></tr><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCreatedAt"><path d="M6.98643729,14.0000972 C5.19579566,14.0000972 3.40419152,13.3106896 2.04245843,11.9323606 C-0.681017475,9.21200555 -0.680780251,4.76029539 2.04293482,2.04012507 C4.76664406,-0.68004331 9.22427509,-0.68004331 11.9480135,2.04013479 C13.272481,3.36277455 14,5.1330091 14,6.99552762 C14,8.87640182 13.2721894,10.6285043 11.9480135,11.9509302 C10.5679344,13.3105924 8.77756503,14.0000972 6.98643729,14.0000972 Z M10.2705296,7.00913883 L10.2705296,8.46099754 L10.2705296,8.65543362 L10.076181,8.65543362 L8.6543739,8.65543362 L5.72059514,8.65543362 L5.52619796,8.65543362 L5.52619796,8.46099754 L5.52619796,5.52541044 L5.52619796,3.37946773 L5.52619796,3.18502193 L5.72059514,3.18502193 L7.17253164,3.18502193 L7.36692883,3.18502193 L7.36692883,3.37946773 L7.36692883,6.81467358 L10.076181,6.81467358 L10.2705296,6.81467358 L10.2705296,7.00913883 Z M12.1601539,6.99552762 C12.1601539,5.61697497 11.6190112,4.32597154 10.6393933,3.34769528 C8.63253764,1.34336744 5.35197452,1.34061603 3.34153136,3.33944106 C3.33868273,3.34219247 3.33607716,3.34494388 3.33322852,3.34769528 C1.32397148,5.35459953 1.32372842,8.63641682 3.33322852,10.6433794 C5.34295224,12.6504489 8.62968901,12.6504489 10.6393933,10.6433794 C11.6190112,9.66506426 12.1601539,8.37408027 12.1601539,6.99552762 Z"></path></svg></span>Created</th><td><time>@Sep 20, 2020 6:18 AM</time></td></tr><tr class="property-row property-row-file"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesFile"><path d="M5.94578,14 C4.62416,14 3.38248,13.4963 2.44892,12.585 C1.514641,11.6736 1,10.4639 1,9.17405 C1.00086108,7.88562 1.514641,6.67434 2.44892,5.76378 L7.45612,0.985988 C8.80142,-0.327216 11.1777,-0.332396 12.5354,0.992848 C13.9369,2.36163 13.9369,4.58722 12.5354,5.95418 L8.03046,10.2414 C7.16278,11.0877 5.73682,11.0894 4.86024,10.2345 C3.98394,9.37789 3.98394,7.98769 4.86024,7.1327 L6.60422,5.4317 L7.87576,6.67196 L6.13177,8.37297 C6.01668,8.48539 6.00003,8.61545 6.00003,8.68335 C6.00003,8.75083 6.01668,8.88103 6.13177,8.99429 C6.36197,9.21689 6.53749,9.21689 6.76768,8.99429 L11.2707,4.70622 C11.9645,4.03016 11.9645,2.91757 11.2638,2.23311 C10.5843,1.57007 9.40045,1.57007 8.72077,2.23311 L3.71342,7.0109 C3.12602,7.58406 2.79837,8.35435 2.79837,9.17405 C2.79837,9.99459 3.12602,10.7654 3.72045,11.3446 C4.90947,12.5062 6.98195,12.5062 8.17096,11.3446 L10.41911,9.15165 L11.6906,10.3919 L9.4425,12.585 C8.50808,13.4963 7.2664,14 5.94578,14 Z"></path></svg></span>Materials</th><td><span style="margin-right:6px"><a href="https://www.coursera.org/learn/deep-neural-network/home/week/1">https://www.coursera.org/learn/deep-neural-network/home/week/1</a></span></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>Property</th><td><a href="https://www.coursera.org/learn/neural-networks-deep-learning/" class="url-value">https://www.coursera.org/learn/neural-networks-deep-learning/</a></td></tr><tr class="property-row property-row-checkbox"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCheckbox"><path d="M0,3 C0,1.34314 1.34326,0 3,0 L11,0 C12.6567,0 14,1.34314 14,3 L14,11 C14,12.6569 12.6567,14 11,14 L3,14 C1.34326,14 0,12.6569 0,11 L0,3 Z M3,1.5 C2.17139,1.5 1.5,2.17157 1.5,3 L1.5,11 C1.5,11.8284 2.17139,12.5 3,12.5 L11,12.5 C11.8286,12.5 12.5,11.8284 12.5,11 L12.5,3 C12.5,2.17157 11.8286,1.5 11,1.5 L3,1.5 Z M2.83252,6.8161 L3.39893,6.27399 L3.57617,6.10425 L3.92334,5.77216 L4.26904,6.10559 L4.44531,6.27582 L5.58398,7.37402 L9.28271,3.81073 L9.45996,3.64008 L9.80664,3.3056 L10.1538,3.63989 L10.3311,3.81067 L10.8936,4.35303 L11.0708,4.52399 L11.4434,4.88379 L11.0708,5.24353 L10.8936,5.41437 L6.1084,10.0291 L5.93115,10.2 L5.58398,10.5344 L5.23682,10.2 L5.05957,10.0292 L2.83057,7.87946 L2.65283,7.70801 L2.27832,7.34674 L2.6543,6.98694 L2.83252,6.8161 Z"></path></svg></span>Reviewed</th><td><div class="checkbox checkbox-off"></div></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Type</th><td><span class="selected-value select-value-color-yellow">Section</span></td></tr></tbody></table></header><div class="page-body"><h1 id="fc99dd83-fb03-455b-830a-202c38f11ea2" class="">Setting up your Machine Learning Application</h1><p id="5b317926-2ab5-44fe-84b7-a95dc6110326" class="">
</p><h2 id="7e9bf07a-55cb-410e-a7d5-a7d9bc4f22a7" class="">Train / Dev / Test sets</h2><p id="16e0c490-a0fb-4ed9-9384-aba6b81cd388" class="">When starting off a machine learning project you need to make a lot of decisions such as:</p><ul id="e804e363-8660-4cc8-9437-de09920880d7" class="bulleted-list"><li>The number of layers</li></ul><ul id="7b1bd32a-1095-44b0-952e-96ffeeb16ffa" class="bulleted-list"><li>The number of hidden layers each layer should have</li></ul><ul id="f9597af4-9e1e-4f0b-8d89-ae48587dbe0c" class="bulleted-list"><li>The type of activation for your project</li></ul><ul id="36b62034-5c42-4553-b10a-2222b90d587a" class="bulleted-list"><li>The learning rate</li></ul><p id="40da8e92-fcfe-4316-9300-e3ab46e14374" class="">
</p><p id="dbb76a06-6232-4355-a604-25aec9c7cf04" class="">But this doesn&#x27;t come easy as ML is a highly interactive process, it takes a while until you find optimised solutions but one of the things that will determine how quickly you make progress is how efficiently you can go around the process cycle.</p><p id="b29893ca-25fe-4e3f-ba32-a89c3eff10dd" class="">
</p><p id="06d06505-fda4-40e4-9e70-4db07db19b81" class="">Setting up your training, cross-validation and test set can go a long way as adjusting your sets could get you highly optimised solution...</p><p id="8593848f-9145-45dd-b041-ee881054e812" class="">
</p><p id="c1aee40e-c16d-43af-b92c-208edd414c5b" class="">â†’ One thing to note is to make sure that your <strong>cross-validation</strong> and <strong>test</strong> set comes from the same distribution as this might influence your end product.</p><p id="98bb63e3-02f6-44bc-b7f5-8895050f2699" class="">â†’ It is also <strong>okay</strong> not to have a test set as long as the <strong>cross-validation</strong> set is available (as x-validation is technically testing).</p><h2 id="f0fcf111-9f11-4fc6-b014-848395a98aa8" class="">Bias / Variance</h2><p id="e82f9bf0-099d-4ef9-8c8b-cf59a30260b0" class="">
</p><figure id="bbcd7c94-b153-4340-ba14-8b7f43fbaec9" class="image"><a href="Week%201/Untitled.png"><img style="width:784px" src="Week%201/Untitled.png"/></a></figure><p id="3f918b38-83cf-4c6f-a044-a03ec0019b9c" class=""><strong>Bias </strong>refers to the difference between your model&#x27;s expected predictions and the true values and, <strong>Variance </strong>refers to your algorithm&#x27;s sensitivity to specific sets of training data.</p><p id="c3700daf-fe49-4454-b847-0720b5588e4d" class="">
</p><p id="cb8f82b9-b1b2-4446-8430-05d287923bd7" class="">From the image above, imagine fitting a linear/logistic regressing to a dataset that has a non-linear pattern. A linear/logistic regression will not be able to model the curves in the data. This is known as <strong>Under-fitting. </strong>occurs when there&#x27;s <strong>high bias.</strong></p><p id="c441225f-470c-4a96-bba6-6aaf27ec964b" class="">Now with the same data, imagine our algorithm fors completely unconstrained, super-flexible to the same dataset. This is known as <strong>Over-fitting </strong>occurs when there&#x27;s <strong>high variance.</strong></p><p id="ab38ccf6-58db-40f6-a9d4-38f13627adb8" class="">
</p><p id="ff6f2e1a-a84c-48d9-8ee7-c8ed1b2a4901" class="">But there might be a classifier in between with a medium level of complexity that fits correctly we refer to this as <strong>Just right </strong>which simplly means that theres <strong>low bias and low varience</strong>.</p><p id="c5e3836b-7770-4a7a-88b9-9a3a7fd20f42" class="">
</p><p id="55d08c3d-3c4e-49ed-98cf-423d57421460" class="">Ref: <a href="https://elitedatascience.com/bias-variance-tradeoff">https://elitedatascience.com/bias-variance-tradeoff</a></p><p id="9297e4f3-0071-4b13-bc4b-8cca99b0fcd5" class="">
</p><figure id="d91b618f-5d0c-4fe5-85dc-19af826a340b" class="image"><a href="Week%201/Untitled%201.png"><img style="width:783px" src="Week%201/Untitled%201.png"/></a></figure><p id="2636a510-4692-4910-aaea-415c21b0b0f7" class="">
</p><p id="d3936195-fa43-4141-938c-2cffdf78d9a0" class="">Key takeaway:</p><ul id="cd05e295-e0a4-4c40-af86-3504a46ed95c" class="bulleted-list"><li>By looking at the <em>Train set error rate</em> you can get a sense of how well your model is perfoming on the training set and that would tell you if you have a <em><strong>bias</strong></em> problem or not and,</li></ul><ul id="5d0dd511-041a-4491-891f-d0d87905f1f1" class="bulleted-list"><li>Looking at how much higher your error goes from training set to cross-validation<em> set, </em>that should give you a sense of how bad is the <strong><em>varience</em></strong> problem.</li></ul><ul id="7605a3d0-1873-4efe-a836-1c54fd7111c6" class="bulleted-list"><li>However, the error rates are based on human perception of error which is assumed to be 0% error this is also called the Optical/ <a href="https://en.wikipedia.org/wiki/Bayes_error_rate">Bayes error</a></li></ul><p id="dbe12cce-d561-45a8-b1b1-6acb2b89c730" class="">
</p><figure id="45c1f5b8-b79f-46de-bb28-c530ec698b61" class="image"><a href="Week%201/Untitled%202.png"><img style="width:783px" src="Week%201/Untitled%202.png"/></a></figure><p id="e32b8bc7-9498-4f2f-833a-305693fb3bfa" class="">In the case of your model <strong>under-fitting and over-fitting </strong>at the same time, were  it has a high bias by being a linear/logaristic classifier is not <em>fitting</em> and being flexible in the middle this would be an example of <em>overfitting.</em></p><h2 id="18685184-da30-47ba-8103-5d02e8a8558c" class="">Basic recipe for Machine Learning</h2><p id="20e48edf-6cad-4444-ae6d-d9aaa9cd0993" class="">
</p><figure id="9d02369f-023b-40f9-a303-a2cf4cf896e7" class="image"><a href="Week%201/Untitled%203.png"><img style="width:782px" src="Week%201/Untitled%203.png"/></a></figure><p id="a3fd2ce5-c577-419a-a886-154eff713aaf" class="">In the previous section we saw how observing our <em>training error</em> and<em> cross-validation error </em>can help diagnose whether our algorithm has a bias or a varience problem or both.</p><p id="fb66221d-4ddd-4ffe-9b4b-36c1dcadcca6" class="">
</p><p id="07e477e6-8f55-4c19-84e4-7220195380d5" class="">Ways of improving your algorithms perfomance (this is an interative process):</p><ul id="284d1e6c-dda7-466f-9515-6e16e39ebad7" class="bulleted-list"><li>If initial model has <strong>High Bias;</strong><ul id="2592a0a6-119c-47b5-aaec-4d4a064d3182" class="bulleted-list"><li>Look at the training data set perfomance.</li></ul><ul id="b766c3ba-0e25-4fde-837f-9d3dbbf0d000" class="bulleted-list"><li>Increase your NN (more layers and more hidden layers)</li></ul><ul id="1c6ed86c-da15-4031-8b87-96b7cf6f5291" class="bulleted-list"><li>Increase time to train.</li></ul></li></ul><ul id="ed1ac0aa-e8e3-4978-af3a-fb0567b1dcf5" class="bulleted-list"><li>Once<strong> </strong>the <strong>Bias</strong> has been removed, evaluate whether you have <strong>High Variance</strong> (if yes);<ul id="da821c34-e6f8-427e-949c-059b2fcf0e70" class="bulleted-list"><li>Collect more training data</li></ul><ul id="0699ffda-2a70-4dc9-8e0e-d7e1c41009dc" class="bulleted-list"><li>Regularisation in order to reduce over fitting.</li></ul><ul id="89c10cfc-53d1-4824-9e84-1c11127f33d9" class="bulleted-list"><li>An alternative NN architecture</li></ul></li></ul><h2 id="a5c0c2a6-b4c9-4684-beab-0e8120c73288" class="">Regularizing your neural network</h2><p id="66652623-dcc8-4cb8-9fd4-50895dbf1216" class="">If you suspect that your networks is overfitting your data, you might have a <strong>high variable problem, </strong>one of the firsth things you should consider is <strong>regularization.</strong></p><p id="c25b354b-8e14-4770-80ef-daa2266baecc" class="">From wikipedia, <a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29">Regularization</a> is the process of adding information in the order to solve or prevent overflitting. </p><p id="1b1f5cea-6b79-476a-bf61-1a34f6760b3e" class="">Great explaination with analogies for better understanding: <a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture">https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture</a></p><p id="8ef4bb55-05ff-46f7-a583-7688b14b614a" class="">
</p><figure id="2854816d-701b-42ff-b5b8-4241e12114b5" class="image"><a href="Week%201/Untitled%204.png"><img style="width:1242px" src="Week%201/Untitled%204.png"/></a></figure><p id="a29959bb-5524-440a-afb6-774a174359c5" class="">Recall that in logistic regression, you are trying to minimise the <em>cost function </em><em><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(w,b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span></em><em>. </em>So in order to minimize overfitting on regressiion problems you would need to add the regularization parameter <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mi mathvariant="normal">/</mi><mn>2</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">\lambda/2m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Î»</span><span class="mord">/</span><span class="mord">2</span><span class="mord mathdefault">m</span></span></span></span></span><span>ï»¿</span></span> with an efficient reqularization technique (preferably L2 regulatization) - this parameter is part of the hyperparameters and the value depends on the problem.</p><p id="966c4234-b05a-4806-8bc0-264bd7eeeeb4" class="">
</p><p id="68ff400c-fb15-4d62-bb9e-0b09a6d7f522" class="">There are three efficient regularizarion techniques, namely:</p><ul id="bb6d2946-190f-4acc-892d-3226de9600ab" class="bulleted-list"><li>Dropout (See next section)</li></ul><ul id="dfa077e7-70fd-4694-b974-03807b131704" class="bulleted-list"><li><em>Euclidian (L2) regularization this is the most common type of regularization which penalizes weights in propotion to the sum of the squares of the weights. It helps drive weight vectors closer to 0 but not quite 0 (as compared to L1) which is also refered to as </em><em>weight decay</em><p id="5f81a951-6b75-40b9-82fc-6d973adb9d90" class=""><em> </em><a href="https://www.notion.so/The-Best-Artificial-Intelligence-Machine-Learning-and-Data-Science-Resources-21d037ac80c24e32a89bb5f36d488bed"><span class="icon">ðŸ¤–</span>The Best Artificial Intelligence, Machine Learning and Data Science Resources*</a>  . And is denoted by the formula below.</p></li></ul><p id="b8071be6-5b36-4451-9be7-a8b9ad64b04a" class="">
</p><figure id="4a31d1b2-5a28-4184-85b7-7661db45c51a" class="image"><a href="Week%201/Untitled%205.png"><img style="width:592px" src="Week%201/Untitled%205.png"/></a></figure><p id="ef03f130-e31c-42da-ac22-42403bc2cb2d" class="">
</p><ul id="c4270de3-13e0-4421-81a5-19dafd2e429c" class="bulleted-list"><li><em>L1 regularization</em> constrasts with L2 regularization, this type of regularization penalises weights in proportion to the sum of the absolute values of the weights.<ul id="ac5a96da-9a5b-4de1-a5e7-0e33f1e8cbd4" class="bulleted-list"><li><strong>Note:</strong> If you use L1 regularization your weights will be sparse i.e some of your weight vectors will comprise of zeros thus compressing your model (save memory) which might only help a little.</li></ul><p id="a6e03bb5-5dac-48fc-8bbb-b94517e61330" class="">
</p><figure id="8b6b2be4-b242-47da-a2fa-34a8c6535a61" class="image"><a href="Week%201/Untitled%206.png"><img style="width:614px" src="Week%201/Untitled%206.png"/></a></figure><p id="5807f0e0-fd59-43e5-8d53-d332fd5e6f02" class="">
</p></li></ul><p id="c4f1d6fd-fd37-405c-96ac-e280acf69959" class="">
</p><p id="296f160f-1f43-42a1-ae18-bc59cd092950" class="">When computing the regularization on a neural network we <em>add lambda over the sum all W parameters and the w parameter matrix which is refered to as </em><em>squared norm.</em><em> </em>Where the squared norm is the L2 regularization. Computing gradient descent with L2 regularization is just a mere adding the <em>regularization parameter and matrix norm</em> <em>(highlighted in red)</em></p><figure id="5c2ca26a-cb5d-449f-b236-34b11c13b46e" class="image"><a href="Week%201/Untitled%207.png"><img style="width:1244px" src="Week%201/Untitled%207.png"/></a></figure><p id="55c2186e-26f6-4eab-9e4c-001790ee4924" class="">
</p><p id="fa8fe33c-d69f-45bc-9ba3-7a45781669c5" class="">Note: When setting up regularization parameters (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">Î»</span></span></span></span></span><span>ï»¿</span></span>), if regularization is large then the weight parameters <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">m</span></span></span></span></span><span>ï»¿</span></span> will be very small thereby affecting the value of Z making it very small. The neural network therefore functions as a very complex linear regression.</p><p id="bbedec2f-f3a9-4769-972e-24fa08d63e83" class="">
</p><figure id="4e5a7ead-079a-46e0-a98f-426ee1618fa1" class="image"><a href="Week%201/Untitled%208.png"><img style="width:1139px" src="Week%201/Untitled%208.png"/></a></figure><p id="d0072a9d-3b6f-4a82-b654-c108c867f997" class="">
</p><p id="f502b0d1-2c67-41d3-ab50-6c4c46516e94" class=""><strong>What you should remember</strong> -- the implications of L2-regularization on:</p><ul id="7e606725-50b7-488e-a91d-02d84f5e5747" class="bulleted-list"><li>The cost computation: <ul id="cfd4f105-deb0-4999-83c3-9969a24042d5" class="bulleted-list"><li>A regularization term is added to the cost</li></ul></li></ul><ul id="4fe83a3b-29b7-424a-bf6a-42c6b0c751ea" class="bulleted-list"><li>The backpropagation function:<ul id="fb1fc221-9b5f-43dc-82af-ac82d869c044" class="bulleted-list"><li>There are extra terms in the gradients with respect to weight matrices</li></ul></li></ul><ul id="003d1f47-5e45-498b-893f-8b58589ccf2c" class="bulleted-list"><li>Weights end up smaller (&quot;weight decay&quot;): <ul id="b3e994df-d934-42c4-907a-48f840f76fcc" class="bulleted-list"><li>Weights are pushed to smaller values.</li></ul></li></ul><p id="a1a68fed-26aa-4277-bc68-31c86fa98307" class="">
</p><p id="351927bc-4e8d-47db-82cc-dbc8e52867e4" class="">Some useful resources: </p><ul id="35a8285f-ef02-4289-ac66-3995d9225045" class="bulleted-list"><li><a href="https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036">https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036</a></li></ul><ul id="ea43da76-6171-4277-bed6-e49ebd3e19ac" class="bulleted-list"><li><a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c</a></li></ul><ul id="f3412ce5-7190-42bb-86de-1267596a9e08" class="bulleted-list"><li><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization">https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization</a></li></ul><p id="9733a7d2-5fb3-469b-a704-9efe21d5e506" class="">
</p><h3 id="d1a2691b-7a65-479b-b13a-d5989ad73d5b" class="">Dropout Regularization</h3><p id="fe542943-9bb2-4e09-9478-8a64a1185e94" class="">This type of regularization, works by removing random selection of fixed number of units in a network layer for a single gradient descent step. See: <a href="https://developers.google.com/machine-learning/glossary#dropout-regularization">https://developers.google.com/machine-learning/glossary#dropout-regularization</a></p><p id="13fd35d2-8e88-4422-940b-5f266374f712" class="">The downside to dropout regularization is that your cost function J becomes redundant as the networks changes at every gradient descent calculation.</p><p id="101c1917-8b1e-4ef8-8485-bc8c2690df21" class="">
</p><figure id="408a30c2-3ef6-4b13-b6ee-c08454b84a48" class="image"><a href="Week%201/Untitled%209.png"><img style="width:1142px" src="Week%201/Untitled%209.png"/></a></figure><p id="cce28064-adf6-4661-9d04-9763d1df2b7a" class="">
</p><p id="27a47722-be96-4fc1-be36-313e1831c5ee" class="">D<strong>ropout</strong> is a widely used regularization technique that is specific to deep learning. <strong>It randomly shuts down some neurons in each iteration.</strong> Watch these two videos to see what this means!</p><figure id="d7a4f1d1-e072-4e25-a211-626082c897ac"><div class="source"><a href="Week%201/dropout1_kiank.mp4">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fe79e9bc-3a4e-44c0-99cd-bc9373ac9037/dropout1_kiank.mp4</a></div></figure><p id="c34deca8-a423-42b8-9d78-8392610cc081" class="">When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p><p id="baa6396e-67f2-4e6d-84cf-72c2e77e785a" class="">
</p><ul id="5e1ca5c8-4e3c-4e46-a9a7-179fb8e0ad47" class="bulleted-list"><li>A <strong>common mistake</strong> when using dropout is to use it
both in training and testing. You should use dropout (randomly eliminate nodes) only in training.</li></ul><ul id="9d635e7b-3df1-40d9-a591-90543b115105" class="bulleted-list"><li>Apply dropout both during forward and backward propagation.</li></ul><ul id="53989174-baef-40f4-92f6-af8222e7c5a8" class="bulleted-list"><li>Deep learning frameworks like <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">tensorflow</a>, <a href="http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html">PaddlePaddle</a>, <a href="https://keras.io/layers/core/#dropout">keras</a> or <a href="http://caffe.berkeleyvision.org/tutorial/layers/dropout.html">caffe</a> come with a dropout layer implementation. Don&#x27;t stress - you will soon learn some of these frameworks.</li></ul><p id="02057479-219d-4aab-846e-bf7c343a8398" class="">
</p><p id="f9818277-e921-4258-912a-fb9bcf5de942" class="">See journal paper: </p><figure id="7a0fc768-d0fb-427e-91d7-041ff22081bb"><div class="source"><a href="https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf</a></div></figure><p id="5e74bb9a-ae0a-4fcb-9ceb-03c5a9e1e115" class="">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p><h3 id="e0fc8fba-838e-4fc6-a75b-20a448dcd23c" class="">Other regularization methods
</h3><p id="bcfa7cc5-bec3-4f77-b18e-9b6b489e7d05" class="">Data augmentation â†’ <a href="https://developers.google.com/machine-learning/glossary#data-augmentation">https://developers.google.com/machine-learning/glossary#data-augmentation</a></p><p id="5f2e147d-18cd-466d-b4b4-6cd3e1ea8cb4" class="">Early stopping â†’ <a href="https://developers.google.com/machine-learning/glossary#early-stopping">https://developers.google.com/machine-learning/glossary#early-stopping</a><div class="indented"><ul id="d11a0d87-b656-406a-ad8f-f761910fb132" class="bulleted-list"><li>Downside for using early stopping, is that when stopping the calculation gradient descent midway this affects the cost function. Alternatively consider using L2 regularization</li></ul><p id="5ddfe7de-c9a0-47ee-98fc-df3ee3434b77" class="">
</p><figure id="bde06ff3-9d4c-4119-b829-e3fea3fff577" class="image"><a href="Week%201/Untitled%2010.png"><img style="width:1146px" src="Week%201/Untitled%2010.png"/></a></figure><p id="669bbf0b-67a8-47e1-b9d2-9aa73e75ff3e" class="">
</p></div></p><h2 id="aeaffd8e-f02e-4fcc-879a-4bdcfa7cf4a2" class="">Setting up your optimization problem</h2><h3 id="39f32046-94e6-4a19-a2f9-ee0c1d6bee22" class="">Normalizing Inputs</h3><p id="d3268119-e8c9-44df-8637-4510c5f220d2" class="">When training a neural network one of the techniques that will speed up the training time is to normalize your inputs, this means that converting range of values into a standard range of values. Typically ranging from -1 to +1 or 0 to 1</p><p id="c8002d4c-eedb-4992-bc18-154aa9e848db" class=""><a href="https://developers.google.com/machine-learning/glossary#normalization">https://developers.google.com/machine-learning/glossary#normalization</a></p><p id="15266ff0-2526-47a8-89e2-ab9d9a7ff59d" class="">
</p><figure id="fb007dc7-bf86-4ea1-a391-def846ebb01c" class="image"><a href="Week%201/Untitled%2011.png"><img style="width:1147px" src="Week%201/Untitled%2011.png"/></a></figure><p id="22aaeb94-4c17-429b-820b-134cf25db29a" class="">
</p><h3 id="1a65d995-9567-4fad-96f1-761acce98661" class="">Vanishing/Exploding gradients</h3><p id="3e709b03-1436-477e-8ef9-ca0119593283" class="">One of the issues when training neural networks is that of vanishing/exploding gradients which means that during training a deep network your derivatives/slope can get very big or very small thus making training difficult.</p><p id="2d004258-ffad-4e54-9fdc-d22fdd6e9188" class="">Detailed explaination: <a href="https://developers.google.com/machine-learning/glossary#vanishing-gradient-problem">https://developers.google.com/machine-learning/glossary#vanishing-gradient-problem</a> and <a href="https://developers.google.com/machine-learning/glossary#exploding-gradient-problem">https://developers.google.com/machine-learning/glossary#exploding-gradient-problem</a></p><p id="efbc72b9-982e-4a1b-80c4-7db012311cb4" class="">
</p><p id="d210311a-2bfe-4e87-9df2-b8a8b7d01a5a" class="">Key takeaway:</p><ul id="ec80b884-b3c4-4774-b503-d8b027209d8c" class="bulleted-list"><li>if your weight vectors are <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span><span>ï»¿</span></span> 1 (identity matrix) then with a deep netwrok the activations can explode.</li></ul><ul id="7c757619-3242-4c2e-bbfb-a63bf960e4c7" class="bulleted-list"><li>However, with weight vectors <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span></span><span>ï»¿</span></span> 1, then the activations will decrease explonentially.</li></ul><p id="695db88c-3b7a-4635-925e-b7c5a30bc24f" class="">
</p><h3 id="33f2ffb5-b934-4542-8506-f6e0a13daa26" class="">Weight initialization for deep networks</h3><p id="52de33c2-5355-4121-ace9-ac1790bfc6c3" class="">To ensure that your network does not have vanishing/exploding gradient you would need to initialize your weights and multiply them with an initialization vector(hyperparameter).</p><p id="9106774e-ce09-4953-beef-f7b4693b3222" class="">
</p><p id="cc920117-018d-492d-b775-8249e2a9cb9e" class="">When your activation function is a ReLU use a multiplier of <code>np.sqrt(2/n^(l-1))</code> and when using <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span></span></span></span></span><span>ï»¿</span></span> user Xavier initialization</p><p id="7319ba6c-4336-43f9-b18c-e7e8f7b2750f" class="">
</p><p id="2aa8d472-212e-436a-851e-cec5bcdb6168" class="">The sole purpose of doing this is just to ensure that the weight vectors are not more than 1 or closer to 0 which will cause vanishing/exploding gradients.</p><p id="f3e56b72-6a41-41dc-8b31-b419130f4801" class="">
</p><figure id="d1c6bb96-15a0-4084-a83b-55b744d1e2f6" class="image"><a href="Week%201/Untitled%2012.png"><img style="width:1138px" src="Week%201/Untitled%2012.png"/></a></figure><p id="0cb2a7fa-a13c-4f6f-8c88-277b7f77eb5f" class="">
</p><p id="e29c40cc-f836-461f-9720-a26955cb7847" class="">Xavier initialization explained: <a href="https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/">https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/</a></p><p id="95f2adbf-0703-4166-b27e-949881ba7ddd" class="">Understanding inititializations techniques and the math behind it: <a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79</a></p><p id="72b1a394-ebcd-4dc2-8072-9fd387cdeded" class="">
</p><h3 id="fff94060-dd83-4142-b766-c5566c5e891d" class="">Numerical approximation of gradients</h3><p id="5cf8146d-7313-48d1-b044-7899dfb4a510" class="">When implementing back propagation there&#x27;s a test called gredient checking that can ensure that back propagation is correct (See the implemetation on the coding exercize). Inorder to do gradient checking we need to numerically approximate computations of gradients.</p><p id="aa911068-8253-4a14-b8bd-f6123a1475c2" class="">You can numerically verify if your function <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span> is a correct implemantation of the derivative of a function <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span></span><span>ï»¿</span></span>,  (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">df(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span> by using the fomula below:</p><p id="65105a21-953a-4c28-a3d5-36ce8c9f4cde" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>Î¸</mi></mrow></mfrac><mi>J</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi>lim</mi><mo>â¡</mo></mo><mrow><mi>Ïµ</mi><mo>â†’</mo><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{d}{d\theta}J(\theta) = \lim_{\epsilon \rightarrow 0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">Î¸</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mop"><span class="mop">lim</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Ïµ</span><span class="mrel mtight">â†’</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>ï»¿</span></span><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>+</mo><mi>Ïµ</mi><mo stretchy="false">)</mo><mo>âˆ’</mo><mi>J</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo>âˆ’</mo><mi>Ïµ</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mi>Ïµ</mi></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{J(\theta+ \epsilon) - J(\theta-\epsilon)}{2 \epsilon}.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathdefault mtight">Ïµ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">Î¸</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">Ïµ</span><span class="mclose mtight">)</span><span class="mbin mtight">âˆ’</span><span class="mord mathdefault mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">Î¸</span><span class="mbin mtight">âˆ’</span><span class="mord mathdefault mtight">Ïµ</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">.</span></span></span></span></span><span>ï»¿</span></span></p><p id="24b9bd11-67d0-4590-85f5-c10fb03855c2" class="">
</p><figure id="0947264c-9cac-4b08-bf0b-d88f7b699e6c" class="image"><a href="Week%201/Untitled%2013.png"><img style="width:791px" src="Week%201/Untitled%2013.png"/></a></figure><p id="1c334d07-6688-461a-ab58-332d05b184e3" class="">
</p><h3 id="85dcffa4-92c5-44f2-a314-d16bc24a90d0" class="">Grad(ient) Checking</h3><p id="a1667aa1-f487-40ec-ac20-935953bf8283" class="">
</p><figure id="3a05052c-b800-4fb1-9eeb-b378c284bc3e" class="image"><a href="Week%201/Untitled%2014.png"><img style="width:801px" src="Week%201/Untitled%2014.png"/></a></figure><p id="c9cd4713-029e-49b1-838d-c76370b435e8" class="">
</p><p id="54ed1de1-5977-4419-ba0c-685b1aed64c5" class="">Example on implemeting gradient checking: <a href="https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64">https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64</a></p><p id="9e9c293a-f094-48f0-99a3-e64c1408535b" class="">
</p><figure id="b78d9469-9892-4924-98bc-8ef35a262dc7" class="image"><a href="Week%201/Untitled%2015.png"><img style="width:797px" src="Week%201/Untitled%2015.png"/></a></figure><p id="fe547b31-ef32-4b01-a917-0ebedcb2ad3a" class="">
</p><p id="42b5ee8b-6f7b-439a-9c6d-150af4c349cc" class=""><strong>Key takeaway:</strong></p><ul id="2d90175e-2f79-446e-83b6-52ff1722cc36" class="bulleted-list"><li>Implemeting gradient checking you would need to compute the euclidien distance between the approximated value and actual value and if it&#x27;s below <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>âˆ’</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-7}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">âˆ’</span><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span></span></span><span>ï»¿</span></span> your back propagation logic is great else there&#x27;s a problem.</li></ul><ul id="78dbbf7f-26b1-4ba0-94d6-b10acca50d63" class="bulleted-list"><li>Don&#x27;t use grad checking in training - only when debugging.</li></ul><ul id="5a514574-3528-497e-8258-6733714885f2" class="bulleted-list"><li>If an algorithm fails grad check, look at components to try to identify a bug.</li></ul><ul id="b58fc31d-5d5d-49c9-9a8f-f95cc16302c0" class="bulleted-list"><li>Remember regularization ( <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>Î¸</mi></mrow><annotation encoding="application/x-tex">d\theta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span></span></span></span></span><span>ï»¿</span></span> is gradient of <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span></span></span></span></span><span>ï»¿</span></span> with respect to <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¸</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span></span></span></span></span><span>ï»¿</span></span>)</li></ul><ul id="b542b6a0-e564-44a9-a2e7-5b3ca7dbbb83" class="bulleted-list"><li>Doesn&#x27;t work with dropout. (alt, turn of dropout<code>keep_prop=1.0</code> compute grad check then turn back on)</li></ul><ul id="4e2ab063-b4f7-4b88-a655-cf2ec25a8013" class="bulleted-list"><li>Run at random initialization, perhaps again after some training.</li></ul><h1 id="c621e850-6153-4536-9083-4824c56d597f" class=""><strong>Something to remember:</strong></h1><ul id="b643eb14-dca0-483f-b23e-99985fc67644" class="bulleted-list"><li>Regularization will help you reduce overfitting.</li></ul><ul id="09e85bff-dbbc-4fa4-95e4-9ba4de8f240d" class="bulleted-list"><li>Regularization will drive your weights to lower values.</li></ul><ul id="227c757e-b3e1-4b39-b6ab-d4a3d5793b6a" class="bulleted-list"><li>L2 regularization and Dropout are two very effective regularization techniques.</li></ul><ul id="7a9edd27-7b02-4f6f-8a6b-8623b4aa5286" class="bulleted-list"><li>Normalizing your inputs will speed up your training time.</li></ul><ul id="ddf23dcd-cc31-417e-87d1-c68f156b3baf" class="bulleted-list"><li>Initialize your weights to avoid vanishing/exploding gradients.</li></ul><p id="f917669e-214e-4d46-8cba-132c9eabb51a" class="">
</p><hr id="875811cf-97a3-4e91-8445-989636abfc95"/><h1 id="b92488d3-5025-40d5-9cb2-5d32dcb9abc6" class="">Q &amp; A</h1><p id="5212c347-0e3f-4f1a-819a-b4da384f7556" class="">
</p><ol id="d8192b31-77c3-40f6-a2c5-dd72d5a5e540" class="numbered-list" start="1"><li>If you have 10,000,000 examples, how would you split the train/dev/test set?<ul id="0f352e1b-ddd1-4944-a6fa-0412f83b121a" class="bulleted-list"><li>98% train . 1% dev . 1% test</li></ul></li></ol><ol id="8a58172b-ddc0-4b07-8888-2b268e9d83a4" class="numbered-list" start="2"><li>The dev and test set should:<ul id="2be27f32-c75d-4671-9b78-87eebfb98c94" class="bulleted-list"><li>Come from the same distribution</li></ul></li></ol><ol id="9849b7b4-9d96-4001-9d84-1020804bfda3" class="numbered-list" start="3"><li>If your Neural Network model seems to have high bias, what of the following would be promising things to try? (Check all that apply.)<ul id="f7850243-f21a-4617-a80c-ddd536a2837d" class="bulleted-list"><li>Make the Neural Network deeper</li></ul><ul id="ac09c133-b88e-43ce-b55e-6de39c2e4f50" class="bulleted-list"><li>Get more test data</li></ul><ul id="2a158190-d0c5-4b2d-83a2-31543fb6069d" class="bulleted-list"><li>Increase the number of units in each hidden layer</li></ul></li></ol><ol id="fcaa6378-5e08-486a-a3dd-b4325d30f85a" class="numbered-list" start="4"><li>You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)<ul id="765cd34a-a7ad-429e-9ec0-719ab678d6eb" class="bulleted-list"><li>Increase the regularization parameter lambda</li></ul><ul id="b455a5b2-68fd-47f6-9d60-257af99089cf" class="bulleted-list"><li>Get more training data</li></ul></li></ol><ol id="8f884ec3-8d8e-4547-95ab-c340f19ea13b" class="numbered-list" start="5"><li>What is weight decay?<ul id="69a45c96-c9c3-402a-9ea4-ee27c661642f" class="bulleted-list"><li>A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.</li></ul></li></ol><ol id="533f17ba-13b8-4309-9285-089d6b89d4ee" class="numbered-list" start="6"><li>What happens when you increase the regularization hyperparameter lambda?<ul id="997189e2-978d-4bfa-bf81-c9110c9ec9c5" class="bulleted-list"><li>Weights are pushed toward becoming smaller (closer to 0)</li></ul></li></ol><ol id="60b21a95-1475-4045-a8ce-5079b11e1576" class="numbered-list" start="7"><li>With the inverted dropout technique, at test time:<ul id="c198e9f6-f00b-4c90-a8be-d9d83647a40c" class="bulleted-list"><li>You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training</li></ul></li></ol><ol id="f5ec68ab-6d48-4c30-a864-8e478375309d" class="numbered-list" start="8"><li>Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply)<ul id="5321ad96-bad3-4748-80a7-76e3bf962244" class="bulleted-list"><li>Reducing the regularization effect</li></ul><ul id="00fdbece-15f9-420c-aae8-1e2c9e66da7c" class="bulleted-list"><li>Causing the neural network to end up with a lower training set error</li></ul></li></ol><ol id="9ce1e7d6-4447-485b-99bd-ce4cba4baedf" class="numbered-list" start="9"><li>Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.)<ul id="9920c66b-45be-4e8f-9e7b-fcc2af43838f" class="bulleted-list"><li>L2 regularization</li></ul><ul id="ae601eec-5beb-400b-86fb-cff972b625c4" class="bulleted-list"><li>Dropout</li></ul><ul id="dbfe478b-e5bb-48d1-b3df-62e120b68849" class="bulleted-list"><li>Data augmentation</li></ul></li></ol><ol id="bb51ce6b-cfff-4a1c-aa1a-5af62de5c7b6" class="numbered-list" start="10"><li>Why do we normalize the inputs xxx?<ul id="b061fbd9-bb92-4c11-9079-eec5b31edf41" class="bulleted-list"><li>It makes the cost function faster to optimize</li></ul></li></ol></div></article></body></html>